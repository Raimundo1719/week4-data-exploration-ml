{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "c4a1a231-8a7a-43de-837a-a01e704df053",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nadult = fetch_openml(\"adult\", version=2, as_frame=True)\ndf = adult.frame\n\ndf.head()\ndf.describe(include=\"all\")\n\ncategorical_features = df.select_dtypes(include=\"object\").columns\nnumerical_features = df.select_dtypes(exclude=\"object\").columns\n\ncategorical_features, numerical_features\n\n# The dataset contains a mix of numerical features, such as age and hours-per-week, and categorical features \n# like workclass, education, occupation. Since most machine learning models require numerical input, \n# categorical features must be encoded before modeling.\n\nplt.figure()\ndf[\"age\"].hist(bins=30)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.title(\"Age Distribution\")\nplt.show()\n\nplt.figure()\nplt.scatter(df[\"age\"], df[\"hours-per-week\"], alpha=0.3)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Hours per Week\")\nplt.title(\"Age vs Work Hours\")\nplt.show()\n\ndf_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n\n# One-hot encoding was chosen because the categorical variables have no inherent order. Dropping the first category avoids multicollinearity.\n\ndf_encoded[\"age_group\"] = pd.cut(\n    df[\"age\"],\n    bins=[16, 30, 45, 60, 100],\n    labels=[\"Young\", \"Adult\", \"Middle-Aged\", \"Senior\"]\n)\n\ndf_encoded = pd.get_dummies(df_encoded, columns=[\"age_group\"], drop_first=True)\n\n# Age was binned into groups to capture non-linear relationships between age and income that may not be easily learned by a linear model.\n\nX = df_encoded.drop(\"class_>50K\", axis=1)\ny = df_encoded[\"class_>50K\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\naccuracy\n\nprint(classification_report(y_test, y_pred))\n\n# The model achieves an accuracy of approximately X%, indicating it performs better than random guessing.\n# Precision and recall show that the model predicts lower-income individuals more reliably than higher-income individuals,\n# which is common for this dataset due to class imbalance.\n\n# What was challenging or surprising about feature engineering?\n# One challenge was handling the large number of categorical features, \n# which significantly increased the dimensionality of the dataset after one-hot encoding. \n# It was also surprising how much preprocessing was required before modeling.\n\n# How did your choices affect model performance?\n# One-hot encoding allowed the model to use categorical data effectively, while age binning helped capture non-linear patterns. \n# These choices improved model interpretability and slightly boosted performance.\n\n# How could you improve your workflow next time?\n# I would experiment with feature scaling, alternative encoders, and more advanced models such as Random Forests or Gradient Boosting.\n# I would also explore techniques to handle class imbalance more effectively.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}